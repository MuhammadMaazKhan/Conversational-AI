{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjVj7AI35sQ+cDSlc6H9Uy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammadMaazKhan/Conversational-AI/blob/main/NLTK/EX01\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Tokenizer </h1>\n",
        "Tokenizers are fundamental components in Natural Language Processing (NLP). They break down text into smaller units called tokens, which can be words, subwords, or characters. This process is crucial for various NLP tasks like text classification, machine translation, and sentiment analysis.\n",
        "\n",
        "<h2>NLTK (Natural Language Toolkit)</h2>\n",
        "<b>Purpose:</b> NLTK is a powerful Python library used for tasks related to natural language processing (NLP), such as tokenization, stemming, lemmatization, parsing, and more.\n",
        "\n",
        "Here's how you can get started with tokenizers in Python using the popular transformers library:\n",
        "\n",
        "<hr>\n",
        "<h3>1. Installation</h3>\n",
        "First, you need to install the transformers library. You can do this using pip:"
      ],
      "metadata": {
        "id": "iRRT4Z7gQoR7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7M8fOkCrKnld",
        "outputId": "964f1970-8f42-4939-ab1f-c3341bf6e32b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The <b>nltk.tokenize</b> module provides tools for splitting text into smaller units, such as words or sentences.\n",
        "\n",
        "# word_tokenize Function\n",
        "**Purpose:** Splits a sentence or paragraph into individual words (tokens).\n",
        "\n",
        "**How it works:** Uses pre-trained models and language rules to break text into tokens. It can handle punctuation, contractions, and special characters effectively.\n",
        "\n",
        "**Input:** A string of text.\n",
        "\n",
        "**Output:** A list of tokens (words and punctuation marks)."
      ],
      "metadata": {
        "id": "btgV-s-6SKjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "1tN_Hr14K1Js"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# About nltk.download('punkt_tab')\n",
        "\n",
        "**Purpose:** Downloads the Punkt tokenizer model, which is essential for functions like word_tokenize and sent_tokenize.\n",
        "\n",
        "**Usage:** Needed for splitting text into words or sentences. Punkt is a popular unsupervised machine learning tokenizer trained on English and other languages. Fetches the Punkt tokenizer models and stores them locally in the NLTK data directory.Enables functions like word_tokenize and sent_tokenize to work properly."
      ],
      "metadata": {
        "id": "YIfQkf6mTLrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R76E3aV9LiLN",
        "outputId": "a5a5dbf1-6aae-4b2f-a809-6ab534e105b4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# word_tokenize(text) Overview\n",
        "The word_tokenize function from the nltk.tokenize module is used to split a string of text into individual tokens (words, punctuation, etc.). It is a fundamental step in Natural Language Processing (NLP) tasks.\n",
        "\n",
        "***How It Works***\n",
        "\n",
        "**Tokenization**: Breaks down a sentence or paragraph into words and punctuation marks.\n",
        "\n",
        "**Language Rules:** Uses NLTK's Punkt tokenizer model, which is trained on large datasets, to handle language-specific rules like contractions and abbreviations.\n"
      ],
      "metadata": {
        "id": "o3NY315GT_Jh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Artificial Intelligence (AI) is transforming industries worldwide.\n",
        "Its applications range from natural language processing to computer vision,\n",
        "helping automate tasks and improve efficiency.\n",
        "As AI technologies advance, ethical considerations become increasingly important.\"\"\"\n",
        "\n",
        "tokenized = word_tokenize(text)\n",
        "print(tokenized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPPsEvHUK8cg",
        "outputId": "b1acba53-6118-4114-e563-dada3f8dd6c6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Artificial', 'Intelligence', '(', 'AI', ')', 'is', 'transforming', 'industries', 'worldwide', '.', 'Its', 'applications', 'range', 'from', 'natural', 'language', 'processing', 'to', 'computer', 'vision', ',', 'helping', 'automate', 'tasks', 'and', 'improve', 'efficiency', '.', 'As', 'AI', 'technologies', 'advance', ',', 'ethical', 'considerations', 'become', 'increasingly', 'important', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_count = len(tokenized)\n",
        "print(\"Number of Tokens:\", token_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3P078FVL6Gg",
        "outputId": "1283bdc0-79c0-45ff-f43a-c4840f037f87"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Tokens: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FreqDist Overview\n",
        "The FreqDist class in the nltk.probability module is used to calculate the frequency distribution of elements in a sequence, such as tokens (words) from a text.\n",
        "\n",
        "**Purpose:**\n",
        "To count the number of occurrences (frequency) of each element (word, token, etc.) in a given dataset.\n",
        "Commonly used for analyzing the most frequent words in a text.\n",
        "\n",
        "**Parameters:**\n",
        "tokenized_sequence: A list of tokens (e.g., words or symbols) obtained from tokenization.\n",
        "\n",
        "**Returns:**\n",
        "An object of type FreqDist containing the frequency distribution of the tokens."
      ],
      "metadata": {
        "id": "iWJg2sHOUu_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "frequency_distribution = FreqDist(tokenized)\n",
        "\n",
        "\n",
        "print(\"Token Frequency Distribution:\")\n",
        "for token, freq in frequency_distribution.items():\n",
        "    print(f\"{token}: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de5Nl89eMCyr",
        "outputId": "8c3c3d0a-df8b-4a65-c294-81612221864d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token Frequency Distribution:\n",
            "Artificial: 1\n",
            "Intelligence: 1\n",
            "(: 1\n",
            "AI: 2\n",
            "): 1\n",
            "is: 1\n",
            "transforming: 1\n",
            "industries: 1\n",
            "worldwide: 1\n",
            ".: 3\n",
            "Its: 1\n",
            "applications: 1\n",
            "range: 1\n",
            "from: 1\n",
            "natural: 1\n",
            "language: 1\n",
            "processing: 1\n",
            "to: 1\n",
            "computer: 1\n",
            "vision: 1\n",
            ",: 2\n",
            "helping: 1\n",
            "automate: 1\n",
            "tasks: 1\n",
            "and: 1\n",
            "improve: 1\n",
            "efficiency: 1\n",
            "As: 1\n",
            "technologies: 1\n",
            "advance: 1\n",
            "ethical: 1\n",
            "considerations: 1\n",
            "become: 1\n",
            "increasingly: 1\n",
            "important: 1\n"
          ]
        }
      ]
    }
  ]
}